metallica<-c("Lars","James","Jason","Kirk")
metalica
print(metallica)
metallica<-metallica[metallica!= "Jason"]
metallica
metallica<-c(metallica, "Rob")
metallica
friends<-c(friends, "Mille", "Nina", "Kiri", "Jacob")
friends<-c("Mille","Nina","Kiri","Jacob")
friends
friends<-c("Mille","Nina","Kiri","Jacob")
friends<-friends[]
friends<-friends[!= "Jacob"]
friends<-friends[friends!= "Jacob"]
friends
friends<-c(friends, "Jacob")
friends
metallica
metallica
friends
friends<-friends[friends != "Jacob]
"]
friends
friends<-friends[friends != "Jacob"]
friends
friends<-friends[friends!= "Jacob"]
friends
friends<-c(friends, "Jacob")
friends
metallicaNames<-c("Lars","James","Kirk","Rob")
metallicaNames
metallicaAges<-c(47, 47, 48, 46)
metallicaAges
metallica<-data.frame(Name = metallicaNames, Age = metallicaAges)
metallica
metallica$Name
metallica$Age
metallica$childAge<-c(12, 12, 4, 6)
metallica$childAge
metallica
names(metallica)
metallica$fatherhoodAge<- metallica$Age & minus metallica$childAge
metallica$fatherhoodAge<- metallica$Age & - metallica$childAge
fatherhoodAge
metallica
metallica$fatherhoodAge<- metallica$Age - metallica$childAge
metallica
metallicaNames<-c("Lars","James","Kirk","Rob")
metallicaNames
metallicaAges<-c(47, 47, 48, 46)
metallicaAges
metallica<-data.frame(Name = metallicaNames, Age = metallicaAges)
metallica$Age
metallica$childAge<-c(12, 12, 4, 6)
metallica$childAge
metallica
names(metallica)
metallica$fatherhoodAge<- metallica$Age - metallica$childAge
name<-c("Ben", "Martin", "Andy", "Paul", "Graham", "Carina", "Karina", "Doug", "Mark", "Zoe")
husband<-as.Date(c("1973-06-21", "1970-07-16", "1949-10-08", "1969-05-24"))
wife<-as.Date(c("1984-11-12", "1973-08-02", "1948-11-11", "1983-07-23"))
agegap<-husband-wife
agegap
birth_date<-as.Date(c("1977-07-03", "1969-05-24", "1973-06-21","1970-07-16", "1949-10-10", "1983-11-05", "1987-10-08", "1989-09-16","1973-05-20", "1984-11-12"))
job<-c(1,1,1,1,1,2,2,2,2,2)
job
job<-factor(job, levels = c(1:2), labels = c("Lecturer", "Student"))
job<-gl(2, 5, labels = c("Lecturer", "Student"))
levels(job)
levels(job)<-c("Medical Lecturer", "Medical Student")
friends<-c(5,2,0,4,1,10,12,15,12,17)
alcohol<-c(10,15,20,5,30,25,20,16,17,18)
income<-c(20000,40000,35000,22000,50000,5000,100,3000,10000,10)
neurotic<-c(10,17,14,13,21,7,13,9,14,13)
lecturerData<-data.frame(name,birth_date,job,friends,alcohol,income,neurotic)
lecturerData
lecturerPersonality <- lecturerData[, c("friends", "alcohol","neurotic")]
lecturerPersonality
lecturerOnly <- lecturerData[job=="Medical Lecturer",]
lecturerOnly
alcoholPersonality <- lecturerData[alcohol > 10, c("friends","alcohol", "neurotic")]
alcoholPersonality
alcoholPersonalityMatrix <- as.matrix(alcoholPersonality)
alcoholPersonalityMatrix
alcoholPersonalityMatrix <- as.matrix(lecturerData[alcohol > 10,c("friends", "alcohol", "neurotic")])
alcoholPersonalityMatrix
satisfactionData = read.delim("Honeymoon Period.dat", header = TRUE)
satisfactionData = read.delim("Honeymoon Period.dat", header = TRUE)
satisfactionData = read.delim("Honeymoon Period.dat.txt", header = TRUE)
setwd("~/Users/FlowersnIce-cream/Google Drev/Hogwarts/R Studio")
#Load data
search<- read.csv("~/Users/FlowersnIce-cream/Downloads/0/03_visual_search_stuff/visual_search_data/Sebber (2017-09-20 14-30-47).csv", sep=";")
#Load data
search<- read.csv("~Users/FlowersnIce-cream/Downloads/0/03_visual_search_stuff/visual_search_data/Sebber (2017-09-20 14-30-47).csv", sep=";")
#Load data
search<- read.csv("Users/FlowersnIce-cream/Downloads/0/03_visual_search_stuff/visual_search_data/Sebber (2017-09-20 14-30-47).csv", sep=";")
#Load data
search<- read.csv("/Users/FlowersnIce-cream/Downloads/0/03_visual_search_stuff/visual_search_data/Sebber (2017-09-20 14-30-47).csv", sep=";")
'% accuracy:'
mean(search$correct_resp,na.rm = TRUE)*100
#Load data
search<- read.csv("/Users/FlowersnIce-cream/Downloads/0/03_visual_search_stuff/visual_search_data/Sebber (2017-09-20 14-30-47).csv", sep=";")
'% accuracy:'
mean(search$correct_resp,na.rm = TRUE)*100
#Remove NAs
search<-subset(search,search$rt!="NA")
search<-subset(search,search$correct_resp!=0)
#turn variables into factors
search$conjunct<-as.factor(search$conjunct)
search$present<-as.factor(search$present)
#Remove outliers
#search<-subset(search,search$rt<mean(search$rt)+3*sd(search$rt))
#histogram
hist(search$rt,breaks=10)
#Q-Q-plot
qqnorm(search$rt)
#make a log-transformation
search$logrt=log(search$rt)
#histogram
hist(search$logrt,breaks=10)
#Q-Q-plot
qqnorm(search$logrt)
search_model<-lm(logrt~setsize*conjunct*present, data=search)
summary(search_model)
library(ggplot2)
search$setsize_f<-as.factor(search$setsize)
ggplot(search, aes(x = setsize , y = rt, color=conjunct, fill=present)) +
geom_point() + labs(x = "setsize", y = "Response time)") +
geom_smooth(method='lm')
View(search)
sarah<- c(1.95,1.58,1.70,2.46,2.27,2.62,3.32,3.51,3.89,3.41)
mother<-c(3.21,4.04,3.30,3.85,4.13,4.59,4.11,4.29,5.82,5.14)
mean_sarah<-mean(sarah)
mean_mother<-mean(mother)
error_sarah<-sarah-mean_sarah
error_mother<-mother-mean_mother
error_sarah*error_mother
sum_of_scores<- sum(error_sarah*error_mother)
sum_of_scores
cov<-sum_of_scores/9
cov
SD_Sarah<-sqrt(mean_sarah)
cov
Correlation_coefficient<-cov/(0.82*0.79)
Correlation_coefficient
Correlation_coefficient*Correlation_coefficient
roh_squared<-Correlation_coefficient*Correlation_coefficient
Correlation_coefficient_aka_roh<-cov/(0.82*0.79)
install.packages("ggplot2"); install.packages("pastecs"); install.packages ("WRS")
library(ggplot2); library(pastecs); library(WRS)
install.packages ("WRS")
knitr::opts_chunk$set(echo = TRUE)
# First we start out by loading the packages we'll use:
library(tidyverse) ; library(stringr) ; library(stringi) ; library(pastecs) ; library(WRS2)
# Then we set the Working Directory
setwd("/Users/FlowersnIce-cream/Google Drev/Hogwarts/R Studio/Portfolio 3")
# First we start out by choosing a set of data from our reading experiment - we chose Thomas's data
data <- read.csv("logfile_Thomas Hagen Hansen2.csv", sep = ",")
# We start out making a qqplot over reactiontime
qqnorm(data$Reaction.Time, main = "QQ-Plot of over Reactiontime")
# it doesn't look normally distributed
#We'll try to get a better look by making a histogram over reactiontime
ggplot(data, aes(x = Reaction.Time))+
geom_histogram(aes(y = ..density..))+
stat_function(fun = dnorm,
args = list(mean = mean(data$Reaction.Time),
sd = sd(data$Reaction.Time)), color = "orange")+
labs(title = "Histogram with reaction time and condition")
#This also shows us that the data aren't "really" normally distributed
# At last we'll use the stat.desc function:
round(stat.desc(data$Reaction.Time, basic = FALSE, norm = TRUE),3)
# Which shows us, that yes. We suffer from both kurtosis and positive skew
# We try to do 3 different transformations on our reading data to get it normally distributed
data$log <- log(data$Reaction.Time)
qqnorm(data$log, main = "QQ-Plot of Logarithmic transformation")
data$sqrt <- sqrt(data$Reaction.Time)
qqnorm(data$sqrt, main = "QQ-Plot of Squareroot transformation")
data$RT <- 1/(data$Reaction.Time)
qqnorm(data$RT, main = "QQ-Plot of Reciprocal transformation")
# # The reciprocal qqplot seems fairly normal - We'll go ahead with these transformed data and check to be sure that it's really normally distribute.
#We'll return normal distribution statistics on our reciprocal transformation:
round(stat.desc(data$RT, basic = FALSE, norm = TRUE),3)
#Our skew.2SE and kurt.2SE is -2.537 and  1.211 seperately, which is lower than -1 and higher than 1 seperately, and this means that the skewness and kurtosis is still significantly different from zero
# So.. Good thing we checked! Now we found out that the reciprocal data still wasn't normally distributed, so we'll use the more robust non-parametric spearmans-test in the coming correlational studies
# Correlational study 1 - Reaction Time and Word Length
# First we'll make a new column in our "Thomas" data with the mutate function, and use the stri_length function to count the letters in all the words
#But before we do that we need to remove all the special caracters and make the words Uppercase, so we later on can get some statistics from the 'MRC Psycholinguistic Database'
data$clean_words <- str_replace_all(data$Word, pattern = "[[:punct:]]", replacement = "")
data$clean_words <- toupper(data$clean_words) #make words upper case
data <- mutate(data, Word_length = stri_length(data$clean_words))
# Now we'll make a correlation test to see if the reactiontime and the word length are correlated, and we'll use spearmans method since we didn't get any normal distributions with our earlier transformations of the Thomas data.
cor.test(data$Word_length , data$Reaction.Time, method = "spearman")
# we calculate rho^2
((0.1963504^2)*100)
#as can be seen in the correlation test, p-value is .007 and therefore below .05, so the relation between variables is significant. Rho is .196 which is above 0.1 which means we have a small effect -> Rho^2 is 0.04, which means that word_length captures 4% of the variance in Thomas's reaction times
# We'll now show the relation with a scatterplot and and a linear model.
ggplot(data, aes(x = Word_length, y = Reaction.Time )) +
geom_point()+
geom_smooth(method = lm)
#as seen on the graph, we can see that by increasing word length we increase the reaction time
# Correlational study 2 - Reaction Time and Word Frequency
#Here we make a word_string of our variable clea_words taht we can plug into the UWC psycholinguistic database
word_list <- data$clean_words #turn column into a list
word_list <- unique(word_list) #removes duplicates from text
word_string <-  paste( unlist(word_list), collapse=" ") #put a space in between all elements
word_string
#We copy the string into MRC database with brown-frequency ratings, manually copy it into excel, Use the text to columns function to seperate the data, and finish of saving the output as CSV.
# then we read the CSV file
freq <- read.csv("brown_frequency.csv", header = F, sep = ";",na.strings = "-")
colnames(freq) = c("clean_words", "frequency") # we rename the columns in our new Dataframe
merged_df <-merge(data, freq, by = "clean_words") #then merge the new predictor with the old dataset
#And finish off runing a cor.test on the browns word frequency and Reactiontime
cor.test(x= merged_df$freq, y = merged_df$Reaction.Time, method = "spearman")
# we calculate rho^2
(((-0.1644064)^2)*100)
#it can be seen that p-value (.03) is below .05 which means that the relation is significant. Rho is negative (-0.16) which indicates that with higher frequencyscores follow smaller reactiontimes, furthermore it indicates that we see a little effect. Rho^2 is 2.7, which means that we have incaptured 2.7% of the variance
# We'll now show the relation with a scatterplot and and a linear model.
ggplot(merged_df, aes(x = frequency, y = Reaction.Time )) +
geom_point()+
geom_smooth(method = lm)
#as seen on the graph the reaction time is negatively correlated with frequencyscore
# Correlational study 3 - Reaction Time and Imaginability
#We copy the string into MRC database with imaginability ratings, manually copy it into excel, Use the text to columns function to seperate the data, and finish of saving the output as CSV.
# then we read the CSV file
imaginability <- read.csv("imaginability.csv", header = F, sep = ";",na.strings = "-")
colnames(imaginability) = c("clean_words", "imaginability") #rename the columns
merged_df2 <-merge(data, imaginability, by = "clean_words") #merge new predictor with the old dataset
cor.test(x = merged_df2 $imaginability, y = merged_df2$Reaction.Time, method = "spearman")
# we calculate rho^2
(((0.01679731)^2)*100)
#0.03%
#we can see that p-value is not significant and therefore we must acknowledge that we cannot observe any significant relation between imaginability and reaction time. Furthermore, rho is to small to indicate any effect. rho^2 squared tells us that we only capture 0.03% of the variance
# We'll now show the insignificant relation with a scatterplot and and a linear model.
ggplot(merged_df2, aes(x = imaginability, y = Reaction.Time )) +
geom_point()+
geom_smooth(method = lm)
#although it's not significant a linear model seems to point in the direction that there is a negative correlation between imaginability and reactiontime
# code for the t-test study
#Now we'll import all of our colelcted datafiles from the reading experiment (20 in total)
#First we look trough our Working Directory and make list of filenames
filenames <- list.files(pattern = "logfile*")
t_test_data = data.frame() #create a new empty df for this second part of the portfolio
for (i in filenames) {	#the we loop over the list of files
file = read.csv(i)	#we import the current file
t_test_data = rbind(t_test_data, file[162,])
#and append the 162th line of the current file to our new df 't_test_data'using the rbind function
}
cond1 <- filter(t_test_data, Condition == 1) #now we create new dataframe only with condition 1
cond2 <- filter(t_test_data, Condition == 2)# then only with condition 2
#we want to test if our data are normally distributed
ggplot(t_test_data, aes(x = Reaction.Time))+
geom_histogram(aes(y = ..density..), binwidth = 0.06)+
stat_function(fun = dnorm, args = list(mean = mean(t_test_data$Reaction.Time), sd = sd(t_test_data$Reaction.Time)))+ facet_wrap(~ Condition)
#according to the histogram it seems that we are approching normal distribution in condition 1 but condition 2 seems to be more positively skewed
#we'll try to draw qqplots to see it in another way
qqnorm(cond1$Reaction.Time, main = "QQ-Plot of reactiontime in Condition 1")
qqnorm(cond2$Reaction.Time, main = "QQ-Plot of reactiontime in Condition 2")
#Now let's get some numbers using the stat.desc function
round(stat.desc(cond1$Reaction.Time,basic = FALSE, norm = TRUE),3)
#normtest.p is not significant (> .05), which means that the scores are not significantly different from normal distribution, also skew2SE and kurt2SE are below 1
# Condition 1 is normally distributed!!! Hurray
round(stat.desc(cond2$Reaction.Time,basic = FALSE, norm = TRUE),3)
#normtest.p is significant (<.05), which means that the scores are significantly different from normal distribution.
# For condition 2, we'll now try to use some transformations.
cond2 <-  mutate(cond2, logRT = log(Reaction.Time), sqrtRT = sqrt(Reaction.Time), RT = 1/(Reaction.Time))
# we'll run a few QQplots to check if the transformations worked in our favour
qqnorm(cond2$logRT, main = "QQ-Plot of Logarithmic transformation")
qqnorm(cond2$sqrtRT, main = "QQ-Plot of Squareroot transformation")
qqnorm(cond2$RT, main = "QQ-Plot of Reciprocal transformation")
# We'll include some normal distribution statistics
round(stat.desc(cond2$logRT,basic = FALSE, norm = TRUE),3)
round(stat.desc(cond2$sqrtRT,basic = FALSE, norm = TRUE),3)
round(stat.desc(cond2$RT,basic = FALSE, norm = TRUE),3)
# but non of the transformations show a p-value over .05 - which means that they aren't normally distributed
qqnorm(t_test_data$Reaction.Time, main = "QQ-Plot of reactiontime")
# code for the t-test study
#Now we'll import all of our colelcted datafiles from the reading experiment (20 in total)
#First we look trough our Working Directory and make list of filenames
filenames <- list.files(pattern = "logfile*")
t_test_data = data.frame() #create a new empty df for this second part of the portfolio
for (i in filenames) {	#the we loop over the list of files
file = read.csv(i)	#we import the current file
t_test_data = rbind(t_test_data, file[162,])
#and append the 162th line of the current file to our new df 't_test_data'using the rbind function
}
#we want to test if our data are normally distributed
ggplot(t_test_data, aes(x = Reaction.Time))+
geom_histogram(aes(y = ..density..), binwidth = 0.06)+
stat_function(fun = dnorm, args = list(mean = mean(t_test_data$Reaction.Time), sd = sd(t_test_data$Reaction.Time)))+ facet_wrap(~ Condition)
#according to the histogram it seems that we are approching normal distribution in condition 1 but condition 2 seems to be more positively skewed
#we'll try to draw qqplots to see it in another way
qqnorm(t_test_data$Reaction.Time, main = "QQ-Plot of reactiontime")
#Now let's get some numbers using the stat.desc function
round(stat.desc(cond1$Reaction.Time,basic = FALSE, norm = TRUE),3)
#normtest.p is not significant (> .05), which means that the scores are not significantly different from normal distribution, also skew2SE and kurt2SE are below 1
# Condition 1 is normally distributed!!! Hurray
round(stat.desc(cond2$Reaction.Time,basic = FALSE, norm = TRUE),3)
#normtest.p is significant (<.05), which means that the scores are significantly different from normal distribution.
# For condition 2, we'll now try to use some transformations.
cond2 <-  mutate(cond2, logRT = log(Reaction.Time), sqrtRT = sqrt(Reaction.Time), RT = 1/(Reaction.Time))
# we'll run a few QQplots to check if the transformations worked in our favour
qqnorm(cond2$logRT, main = "QQ-Plot of Logarithmic transformation")
qqnorm(cond2$sqrtRT, main = "QQ-Plot of Squareroot transformation")
qqnorm(cond2$RT, main = "QQ-Plot of Reciprocal transformation")
# We'll include some normal distribution statistics
round(stat.desc(cond2$logRT,basic = FALSE, norm = TRUE),3)
round(stat.desc(cond2$sqrtRT,basic = FALSE, norm = TRUE),3)
round(stat.desc(cond2$RT,basic = FALSE, norm = TRUE),3)
# but non of the transformations show a p-value over .05 - which means that they aren't normally distributed
ggplot(t_test_data, aes(x = Reaction.Time))+
geom_histogram(aes(y = ..density..), binwidth = 0.06)+
stat_function(fun = dnorm, args = list(mean = mean(t_test_data$Reaction.Time), sd = sd(t_test_data$Reaction.Time)))+ facet_wrap(~ Condition)
qqnorm(t_test_data$Reaction.Time, main = "QQ-Plot of reactiontime")
round(stat.desc(t_test_data$Reaction.Time,basic = FALSE, norm = TRUE),3)
# code for the t-test study
#Now we'll import all of our colelcted datafiles from the reading experiment (20 in total)
#First we look trough our Working Directory and make list of filenames
filenames <- list.files(pattern = "logfile*")
t_test_data = data.frame() #create a new empty df for this second part of the portfolio
for (i in filenames) {	#the we loop over the list of files
file = read.csv(i)	#we import the current file
t_test_data = rbind(t_test_data, file[162,])
#and append the 162th line of the current file to our new df 't_test_data'using the rbind function
}
#we want to test if our data are normally distributed
ggplot(t_test_data, aes(x = Reaction.Time))+
geom_histogram(aes(y = ..density..), binwidth = 0.06)+
stat_function(fun = dnorm, args = list(mean = mean(t_test_data$Reaction.Time), sd = sd(t_test_data$Reaction.Time)))+ facet_wrap(~ Condition)
#according to the histogram it seems that we are approching normal distribution in condition 1 but condition 2 seems to be more positively skewed
#we'll try to draw qqplots to see it in another way
qqnorm(t_test_data$Reaction.Time, main = "QQ-Plot of reactiontime")
#Now let's get some numbers using the stat.desc function
round(stat.desc(t_test_data$Reaction.Time,basic = FALSE, norm = TRUE),3)
#normtest.p is not significant (> .05), which means that the scores are not significantly different from normal distribution, also skew2SE and kurt2SE are below 1
# Condition 1 is normally distributed!!! Hurray
round(stat.desc(cond2$Reaction.Time,basic = FALSE, norm = TRUE),3)
#normtest.p is significant (<.05), which means that the scores are significantly different from normal distribution.
# For condition 2, we'll now try to use some transformations.
t_test_data <-  mutate(t_test_data, logRT = log(Reaction.Time), sqrtRT = sqrt(Reaction.Time), RT = 1/(Reaction.Time))
# we'll run a few QQplots to check if the transformations worked in our favour
qqnorm(t_test_data$logRT, main = "QQ-Plot of Logarithmic transformation")
qqnorm(t_test_data$sqrtRT, main = "QQ-Plot of Squareroot transformation")
qqnorm(t_test_data$RT, main = "QQ-Plot of Reciprocal transformation")
# We'll include some normal distribution statistics
round(stat.desc(cond2$logRT,basic = FALSE, norm = TRUE),3)
round(stat.desc(cond2$sqrtRT,basic = FALSE, norm = TRUE),3)
round(stat.desc(cond2$RT,basic = FALSE, norm = TRUE),3)
# but non of the transformations show a p-value over .05 - which means that they aren't normally distributed
qqnorm(t_test_data$logRT, main = "QQ-Plot of Logarithmic transformation")
qqnorm(t_test_data$sqrtRT, main = "QQ-Plot of Squareroot transformation")
qqnorm(t_test_data$RT, main = "QQ-Plot of Reciprocal transformation")
qqnorm(t_test_data$logRT, main = "QQ-Plot of Logarithmic transformation")
qqnorm(t_test_data$sqrtRT, main = "QQ-Plot of Squareroot transformation")
qqnorm(t_test_data$RT, main = "QQ-Plot of Reciprocal transformation")
# code for the t-test study
#Now we'll import all of our colelcted datafiles from the reading experiment (20 in total)
#First we look trough our Working Directory and make list of filenames
filenames <- list.files(pattern = "logfile*")
t_test_data = data.frame() #create a new empty df for this second part of the portfolio
for (i in filenames) {	#the we loop over the list of files
file = read.csv(i)	#we import the current file
t_test_data = rbind(t_test_data, file[162,])
#and append the 162th line of the current file to our new df 't_test_data'using the rbind function
}
#we want to test if our data are normally distributed
ggplot(t_test_data, aes(x = Reaction.Time))+
geom_histogram(aes(y = ..density..), binwidth = 0.06)+
stat_function(fun = dnorm, args = list(mean = mean(t_test_data$Reaction.Time), sd = sd(t_test_data$Reaction.Time)))+ facet_wrap(~ Condition)
#according to the histogram it seems that we are approching normal distribution in condition 1 but condition 2 seems to be more positively skewed
#we'll try to draw qqplots to see it in another way
qqnorm(t_test_data$Reaction.Time, main = "QQ-Plot of reactiontime")
#Now let's get some numbers using the stat.desc function
round(stat.desc(t_test_data$Reaction.Time,basic = FALSE, norm = TRUE),3)
#normtest.p is not significant (> .05), which means that the scores are not significantly different from normal distribution, also skew2SE and kurt2SE are below 1
# Condition 1 is normally distributed!!! Hurray
round(stat.desc(cond2$Reaction.Time,basic = FALSE, norm = TRUE),3)
#normtest.p is significant (<.05), which means that the scores are significantly different from normal distribution.
# For condition 2, we'll now try to use some transformations.
t_test_data <-  mutate(t_test_data, logRT = log(Reaction.Time), sqrtRT = sqrt(Reaction.Time), RT = 1/(Reaction.Time))
# we'll run a few QQplots to check if the transformations worked in our favour
qqnorm(t_test_data$logRT, main = "QQ-Plot of Logarithmic transformation")
qqnorm(t_test_data$sqrtRT, main = "QQ-Plot of Squareroot transformation")
qqnorm(t_test_data$RT, main = "QQ-Plot of Reciprocal transformation")
# We'll include some normal distribution statistics
round(stat.desc(t_test_data$logRT,basic = FALSE, norm = TRUE),3)
round(stat.desc(t_test_data$sqrtRT,basic = FALSE, norm = TRUE),3)
round(stat.desc(t_test_data$RT,basic = FALSE, norm = TRUE),3)
# but non of the transformations show a p-value over .05 - which means that they aren't normally distributed
round(stat.desc(t_test_data$Reaction.Time,basic = FALSE, norm = TRUE),3)
qqnorm(t_test_data$Reaction.Time, main = "QQ-Plot of reactiontime")
round(stat.desc(t_test_data$Reaction.Time,basic = FALSE, norm = TRUE),3)
t_test_data <-  mutate(t_test_data, logRT = log(Reaction.Time), sqrtRT = sqrt(Reaction.Time), RT = 1/(Reaction.Time))
qqnorm(t_test_data$logRT, main = "QQ-Plot of Logarithmic transformation")
qqnorm(t_test_data$sqrtRT, main = "QQ-Plot of Squareroot transformation")
qqnorm(t_test_data$RT, main = "QQ-Plot of Reciprocal transformation")
round(stat.desc(t_test_data$logRT,basic = FALSE, norm = TRUE),3)
round(stat.desc(t_test_data$sqrtRT,basic = FALSE, norm = TRUE),3)
round(stat.desc(t_test_data$RT,basic = FALSE, norm = TRUE),3)
t.test(RT ~ Condition , data = t_test_data)
t.test(RT ~ Condition , data = t_test_data)
ggplot(t_test_data, aes( x = Condition ,y = RT ))+
geom_bar(stat = "summary", fun.y= mean)+
geom_errorbar(stat = "summary", fun.data = mean_se, width = 0.2)
ggplot(t_test_data, aes(x = Condition, y = RT)) + geom_boxplot()
ggplot(t_test_data, aes(x = Condition ,y = RT)) + geom_boxplot()
ggplot(t_test_data, aes(y = RT)) + geom_boxplot() + facet_wrap(~Condition)
ggplot(t_test_data, aes(x=Condition,y = RT)) + geom_boxplot() + facet_wrap(~Condition)
ggplot(t_test_data, aes( x = Condition ,y = RT ))+
geom_bar(stat = "summary", fun.y= mean)+
geom_errorbar(stat = "summary", fun.data = mean_se, width = 0.2)
ggplot(t_test_data, aes(x=Condition,y = RT)) + geom_boxplot() + facet_wrap(~Condition)
ggplot(t_test_data, aes(x=RT,y = RT)) + geom_boxplot() + facet_wrap(~Condition)
View(t_test_data)
ggplot(t_test_data, aes(x=Word,y = RT)) + geom_boxplot() + facet_wrap(~Condition)
ggplot(t_test_data, aes( x = Condition ,y = RT ))+
geom_bar(stat = "summary", fun.y= mean)+
geom_errorbar(stat = "summary", fun.data = mean_se, width = 0.2)
ggplot(t_test_data, aes(x=Word,y = RT)) + geom_boxplot() + facet_wrap(~Condition)
ggplot(t_test_data, aes(x=Word,y = RT)) + geom_boxplot() + facet_wrap(~Condition)
ggplot(t_test_data, aes(x=Word,y = Reaction.Time)) + geom_boxplot() + facet_wrap(~Condition)
ggplot(t_test_data, aes(x = Reaction.Time))+
geom_histogram(aes(y = ..density..), binwidth = 0.06)+
stat_function(fun = dnorm, args = list(mean = mean(t_test_data$Reaction.Time), sd = sd(t_test_data$Reaction.Time)))+ facet_wrap(~ Condition)
t.test(RT ~ Condition , data = t_test_data)
ggplot(t_test_data, aes(x=Word,y = Reaction.Time)) + geom_boxplot() + facet_wrap(~Condition)
t <- -0.55207
df <- 17.527
r <- sqrt((t^2)/(t^2)+df)
r
r^2
r
r <- sqrt((t^2)/((t^2)+df)
r
```
r <- sqrt((t^2)/((t^2)+df)
r
```
r <- sqrt(((t^2)/((t^2)+df))
r
```
r <- sqrt((t^2)/((t^2)+df))
r
r^2
(r^2)*100
t <- ind.t.test$statistic[[1]]
t.test <- t.test(RT ~ Condition , data = t_test_data)
t <- ind.t.test$statistic[[1]]
t <- -0.55207
df <- 17.527
r <- sqrt((t^2)/((t^2)+df))
r
(r^2)*100
by(t_test_data$RT, t_test_data$Condition, stat.desc, basic = FALSE, norm = TRUE)
